# desc2matrix_accum_followup.py user manual

This document explains the up-to-date manual for using `desc2matrix_accum_followup.py`, which converts the description text files generated by the `Makefile` into a structured JSON output containing the traits and corresponding values. The difference between `desc2matrix.py` and this script is that this accumulates the list of the names of traits as it processes the species descriptions. This script is also different from `desc2matrix_accum.py` in that it asks the LLM to tabulate the traits of a few species in JSON to generate the initial list of traits, instead of using only the first species to do this. Finally, this script is different from `desc2matrix_accum_tab.py` in that it asks the LLM a **follow-up** question to ensure the completeness of the JSON output.

## Operation

1. Retrieve the initial list of traits from the first few species descriptions in the provided descfile

The model runs the 'tabulation prompt', which asks the LLM to tabulate the traits of the first few species in the descfile. The resulting names of the traits are isolated and saved. The number of species to use in tabulation can be set by the `--initspnum` option, with a default of 3.

2. Retrieve traits from the other species using a growing list of trait names

The names of traits are then fed into the LLM run extracting the traits of the subsequent species. The model is asked to extract the given list of traits, put 'NA' where a given trait is missing in the description, and add any traits that are not in the list but are mentioned in the species description.

3. Ask the LLM a **follow-up question** after the initial response

After generating the initial JSON response, a follow-up question is asked to the LLM, providing a list of non-stop words in the description that it has omitted and asking to incorporate omitted words that carry botanical information. This generates a new version of the response which usually has more characteristics than the initial response.

4. Update the list of traits if needed

The running list of traits is **only updated if the new list of traits is longer than the previous list**. This is to prevent the list from 'degrading' as the script processes the species. If a particular run fails due to the LLM returning a badly structured output, the list of traits is kept the same for the next species.

## Default prompts

The default prompts are hard-coded in `common_scripts/default_prompts.py`, but can be imported from a text file using the relevant options (see **Arguments**). The tabulation prompt and the general extraction prompt must include one or more of the following special 'markers', as is appropriate:

| Marker | In: | Meaning |
| --- | --- | --- |
| `[DESCRIPTIONS]` | Tabulation prompt | Structured list of species descriptions for initial tabulation |
| `[DESCRIPTION]` | Extraction prompt, Follow-up prompt | Plant description text to compile |
| `[CHARACTER_LIST]` | Extraction prompt, Follow-up prompt | Given list of traits to extract |
| `[MISSING_WORDS]` | Follow-up prompt | List of non-stop words in the plant description that were initially omitted |

### System prompt

See `global_sys_prompt` in `common_scripts/default_prompts.py`.

### Tabulation prompt

See `global_tabulation_prompt` in `common_scripts/default_prompts.py`.

### Extraction prompt

See `global_prompt` in `common_scripts/default_prompts.py`.

### Follow-up prompt

See `global_followup_prompt` in `common_scripts/default_prompts.py`.

## Arguments

| Argument | Description | Required? | Default value |
| --- | --- | --- | --- |
| `descfile` (positional argument) | Path to the flora description file to transcribe | Yes | |
| `outputfile` (positional argument) | Path to the output JSON file | Yes | |
| `--desctype` | Name of the 'type' that contains morphological descriptions in the descfile | Yes | |
| `--sysprompt` | Path to a text file containing the system prompt to use | No | See above |
| `--prompt` | Path to a text file containing the general extraction prompt to use | No | See above |
| `--tabprompt` | Path to a text file containing the tabulation prompt to use | No | See above |
| `--fprompt` | Path to a text file containing the follow-up prompt to use | No | See above |
| `--silent` | If flag is present, suppress command-line output showing progress | No | `None` |
| `--start` | Order ID (starting from 0) of the species in the descfile to start transcribing from | No | `0` |
| `--spnum` | Number of species to transcribe | No | `None` (transcribe entire file) |
| `--initspnum` | Number of species to use in initial tabulation | No | `3` |
| `--model` | Name of the base LLM to use. Specified LLM must be installed and running at `localhost:11434` | No | `llama3` |
| `--temperature` | Model temperature between 0 and 1. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `0.1` |
| `--seed` | Random seed to use for reproducibility. Setting to 0 makes the output random. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `1` |
| `--repeatlastn` | Number of tokens(?) the model looks back to prevent repetition. Set to 0 to prevent this behaviour as default. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | 0 |
| `--numpredict` | Number of tokens for model to generate. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `2048` |
| `--numctx` | Size of the context window used to generate the token See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `32768` |
| `--topk` | Parameter adjusting the degree of 'conservativeness' in the model output. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `None` (set to `40` by Ollama) |
| `--topp` | Parameter adjusting the degree of 'conservativeness' in the model output. See [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) for detailed reference | No | `None` (set to `0.9` by Ollama) |

## Output

The output is a single JSON object with the following keys:

| Key | Description |
| --- | --- |
| `metadata` | Run metadata |
| `data` | List of JSONs containing transcribed characteristics of each species (see below) |
| `charlist_len_history` | A list containing the length of the lists of characteristics used for each species processed |
| `charlist_history` | A list containing the lists of characteristics used for each species processed |

The `metadata` is itself a JSON containing the following keys:

| Key | Description |
| --- | --- |
| `sys_prompt` | System prompt |
| `tab_prompt` | Tabulation prompt |
| `prompt` | Prompt used subsequent to `tab_prompt` to extract the characteristics |
| `f_prompt` | Follow-up prompt |
| `params` | Key-value pairs of parameters used for the model run |
| `mode` | 'Mode' used to generate the output. This is set to `desc2json_accum_followup`. |

The `data` list contains JSON objects for every transcribed species, with the following keys:

| Key | Description |
| --- | --- |
| `coreid` | WFO taxon ID of the transcribed species |
| `status` | `success` for successful parse, `invalid_json` for invalid JSON output, `bad_structure` for JSON that's valid but badly structured. The unsuccessful parse statuses can also have `_followup` at the end, which indicates that the error has ocurred in the response to the follow-up prompt. |
| `original_description` | The original description imported from WFO |
| `char_json` | List of JSONS containing the transcribed characteristics. Each element is structured as `{"characteristic": name of characteristic, "value": value of characteristic}`. This is `null` if the response failed to parse. |
| `failed_str` | Response string from the LLM that failed to parse to JSON. This is `null` if the response successfully parsed. |